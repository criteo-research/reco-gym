{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning from logged bandit feedback\n",
    "\n",
    "One of the most common ways that recommender systems are developed in practice involves building models trained on the historical behaviour of the running policy.  This is to be distinguished from bandit approaches such as upper confidence bound or Thompson sampling or full reinforcement learning as in these systems there is no clear separation between a learning stage and an acting stage.  In the approaches considered here, we first learn a model and then deploy a static model that does not change further.\n",
    "\n",
    "Here we describe a simple supervised approach where we model the probability of a click conditional upon features that are created from a combination of the users' attributes and the recommendation.\n",
    "\n",
    "Contextual bandits that use the inverse propensity score will be investigated in future versions of Reco Gym."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    v  u  r  c   ps\n",
      "0   4  1 -1 -1  NaN\n",
      "1  -1  1  1  0  0.1\n",
      "2  -1  1  8  0  0.1\n",
      "3  -1  1  8  0  0.1\n",
      "4  -1  1  9  0  0.1\n",
      "5  -1  1  1  0  0.1\n",
      "6  -1  1  0  0  0.1\n",
      "7  -1  1  7  0  0.1\n",
      "8  -1  1  4  0  0.1\n",
      "9  -1  1  8  0  0.1\n",
      "10 -1  1  8  0  0.1\n",
      "11  4  1 -1 -1  NaN\n",
      "12  9  1 -1 -1  NaN\n",
      "13  5  1 -1 -1  NaN\n",
      "14 -1  1  3  0  0.1\n",
      "15 -1  1  5  1  0.1\n",
      "16  4  1 -1 -1  NaN\n",
      "17  4  1 -1 -1  NaN\n",
      "18  4  1 -1 -1  NaN\n",
      "19  4  1 -1 -1  NaN\n",
      "20  4  1 -1 -1  NaN\n",
      "21  4  1 -1 -1  NaN\n",
      "22 -1  1  1  0  0.1\n",
      "23 -1  1  9  0  0.1\n",
      "24 -1  1  5  0  0.1\n"
     ]
    }
   ],
   "source": [
    "from pylab import *\n",
    "import gym, reco_gym\n",
    "import pandas as pd\n",
    "\n",
    "from reco_gym import env_1_args\n",
    "\n",
    "env_1_args['random_seed'] = 42\n",
    "\n",
    "env = gym.make('reco-gym-v1')\n",
    "env.init_gym(env_1_args)\n",
    "\n",
    "data = env.generate_data(100)\n",
    "print(data[:25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turn the data into features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = []\n",
    "for ii in range(1, 1 + data.u.max()):\n",
    "    counts_v, _ = histogram([v for v in array(data[data.u == ii].v) if v > -1], range = (0, env.num_products))\n",
    "    sub = data[data.u == ii]\n",
    "    sub = sub[sub.r != -1]\n",
    "\n",
    "    for i, c in zip(range(env.num_products), counts_v):\n",
    "        sub['c_v%d' % (i)] = c\n",
    "    train.append(sub)\n",
    "train = pd.concat(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    v  u  r  c   ps  c_v0  c_v1  c_v2  c_v3  c_v4  c_v5  c_v6  c_v7  c_v8  \\\n",
      "1  -1  1  4  0  0.1     0     0     0     0    11     1     0     0     0   \n",
      "2  -1  1  1  0  0.1     0     0     0     0    11     1     0     0     0   \n",
      "3  -1  1  1  0  0.1     0     0     0     0    11     1     0     0     0   \n",
      "4  -1  1  7  0  0.1     0     0     0     0    11     1     0     0     0   \n",
      "5  -1  1  8  0  0.1     0     0     0     0    11     1     0     0     0   \n",
      "6  -1  1  0  0  0.1     0     0     0     0    11     1     0     0     0   \n",
      "7  -1  1  6  0  0.1     0     0     0     0    11     1     0     0     0   \n",
      "8  -1  1  2  0  0.1     0     0     0     0    11     1     0     0     0   \n",
      "9  -1  1  0  0  0.1     0     0     0     0    11     1     0     0     0   \n",
      "10 -1  1  4  0  0.1     0     0     0     0    11     1     0     0     0   \n",
      "14 -1  1  8  0  0.1     0     0     0     0    11     1     0     0     0   \n",
      "15 -1  1  4  1  0.1     0     0     0     0    11     1     0     0     0   \n",
      "22 -1  1  4  0  0.1     0     0     0     0    11     1     0     0     0   \n",
      "23 -1  1  5  0  0.1     0     0     0     0    11     1     0     0     0   \n",
      "24 -1  1  4  0  0.1     0     0     0     0    11     1     0     0     0   \n",
      "25 -1  1  7  0  0.1     0     0     0     0    11     1     0     0     0   \n",
      "26 -1  1  6  0  0.1     0     0     0     0    11     1     0     0     0   \n",
      "27 -1  1  3  1  0.1     0     0     0     0    11     1     0     0     0   \n",
      "31 -1  1  7  0  0.1     0     0     0     0    11     1     0     0     0   \n",
      "32 -1  1  9  0  0.1     0     0     0     0    11     1     0     0     0   \n",
      "33 -1  1  7  0  0.1     0     0     0     0    11     1     0     0     0   \n",
      "34 -1  1  4  0  0.1     0     0     0     0    11     1     0     0     0   \n",
      "35 -1  1  2  0  0.1     0     0     0     0    11     1     0     0     0   \n",
      "36 -1  1  3  0  0.1     0     0     0     0    11     1     0     0     0   \n",
      "48 -1  2  0  0  0.1     0     0     0     0    10     1    45     2     0   \n",
      "\n",
      "    c_v9  \n",
      "1      1  \n",
      "2      1  \n",
      "3      1  \n",
      "4      1  \n",
      "5      1  \n",
      "6      1  \n",
      "7      1  \n",
      "8      1  \n",
      "9      1  \n",
      "10     1  \n",
      "14     1  \n",
      "15     1  \n",
      "22     1  \n",
      "23     1  \n",
      "24     1  \n",
      "25     1  \n",
      "26     1  \n",
      "27     1  \n",
      "31     1  \n",
      "32     1  \n",
      "33     1  \n",
      "34     1  \n",
      "35     1  \n",
      "36     1  \n",
      "48     3  \n"
     ]
    }
   ],
   "source": [
    "print(train[:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/d.mykhaylov/dev/src/reco-gym/.venv/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "y = train.c\n",
    "X = train.loc[:, ['r'] + list(train.keys()[5:])]\n",
    "# 1. import\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# 2. instantiate model\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# 3. fit \n",
    "lr = logreg.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to check how the Logistic Regression works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product #3 was shown 7 times\n",
      "Test X:  [[3. 0. 0. 0. 7. 0. 0. 0. 0. 0. 0.]]\n",
      "Test Y:  [[0.9800594 0.0199406]]\n",
      "Product #3 was shown 70 times\n",
      "Test X:  [[ 3.  0.  0.  0. 70.  0.  0.  0.  0.  0.  0.]]\n",
      "Test Y:  [[0.98669053 0.01330947]]\n"
     ]
    }
   ],
   "source": [
    "# Check the probability of getting click for Product '3' with 7 observations for that Product.\n",
    "test_X = np.zeros((1, env_1_args['num_products'] + 1))\n",
    "\n",
    "test_X[:, 0] = 3\n",
    "test_X[:, 3 + 1] = 7\n",
    "test_Y = lr.predict_proba(test_X)\n",
    "print(\"Product #3 was shown 7 times\")\n",
    "print(\"Test X: \", test_X)\n",
    "print(\"Test Y: \", test_Y)\n",
    "\n",
    "# Check the probability of getting click for Product '3' with 70 observations for that Product.\n",
    "test_X[:, 3 + 1] = 70\n",
    "test_Y = lr.predict_proba(test_X)\n",
    "print(\"Product #3 was shown 70 times\")\n",
    "print(\"Test X: \", test_X)\n",
    "print(\"Test Y: \", test_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you may see, the more frequently the product is shown, the higher the probability that it will be clicked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create a new Agent that incorporates that logic explicitly. I.e. the agent that calculates a _Probability Score_ of Click for a Product based on how many time the product was shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import choice\n",
    "\n",
    "\n",
    "# define agent class\n",
    "class LoggedFeedBackLogistic:\n",
    "    def __init__(self, env):\n",
    "        # set environment as an attribute of agent\n",
    "        self.env = env\n",
    "        self.organic_views = np.zeros(self.env['num_products'])\n",
    "\n",
    "    def train(self, observations, action, reward, done):\n",
    "        for observation in observations:\n",
    "            self.organic_views[observation[1]] += 1\n",
    "\n",
    "    def offline_train(train):\n",
    "        pass\n",
    "\n",
    "    def act(self, _ = None):\n",
    "        '''act method returns an action based on current observation and past\n",
    "            history'''\n",
    "        prob = self.organic_views / sum(self.organic_views)\n",
    "        action = np.random.choice(self.env['num_products'], p = prob)\n",
    "        return {\n",
    "            'a': action,\n",
    "            'ps': prob[action]\n",
    "        }\n",
    "\n",
    "    def reset(self):\n",
    "        self.organic_views = np.zeros(self.env['num_products'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "ABTestNumberOfUsers = 2000\n",
    "\n",
    "env.init_gym(env_1_args) # Reset the Environment\n",
    "a_data = env.generate_data(ABTestNumberOfUsers, LoggedFeedBackLogistic(env_1_args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    v  u  r  c   ps\n",
      "0   4  1 -1 -1  NaN\n",
      "1  -1  1  4  0  1.0\n",
      "2  -1  1  4  0  1.0\n",
      "3  -1  1  4  0  1.0\n",
      "4  -1  1  4  0  1.0\n",
      "5  -1  1  4  0  1.0\n",
      "6  -1  1  4  0  1.0\n",
      "7  -1  1  4  0  1.0\n",
      "8  -1  1  4  0  1.0\n",
      "9  -1  1  4  0  1.0\n",
      "10 -1  1  4  0  1.0\n",
      "11  4  1 -1 -1  NaN\n",
      "12  9  1 -1 -1  NaN\n",
      "13  5  1 -1 -1  NaN\n",
      "14 -1  1  4  0  1.0\n",
      "15 -1  1  4  1  0.5\n",
      "16  4  1 -1 -1  NaN\n",
      "17  4  1 -1 -1  NaN\n",
      "18  4  1 -1 -1  NaN\n",
      "19  4  1 -1 -1  NaN\n",
      "20  4  1 -1 -1  NaN\n",
      "21  4  1 -1 -1  NaN\n",
      "22 -1  1  4  0  0.5\n",
      "23 -1  1  4  0  0.8\n",
      "24 -1  1  9  0  0.1\n"
     ]
    }
   ],
   "source": [
    "print(a_data[:25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a new Agent that uses a Logistic Regression Model (the one we tested above) without explicit calculation of _Probability Scores_ and compare its behaviour with the Agent that calculates _Probability Scores_ explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BuildLogisticRegressionData(\n",
    "    data,\n",
    "    env,\n",
    "    only_with_clicks = False,\n",
    "    sliding_window_depth = -1\n",
    "):\n",
    "    train_X = []\n",
    "    train_Y = []\n",
    "    \n",
    "    number_of_users = data.u.max()\n",
    "    number_of_products = env['num_products']\n",
    "\n",
    "    for user_id in range(number_of_users):\n",
    "        views = np.zeros((0, number_of_products))\n",
    "        for _, user_datum in data[data['u'] == user_id].iterrows():\n",
    "            if user_datum['c'] == -1:\n",
    "                view = int(user_datum['v'])\n",
    "                tmp_view = np.zeros(number_of_products)\n",
    "            \n",
    "                tmp_view[view] = 1\n",
    "                views = np.append(tmp_view[np.newaxis, :], views, axis = 0)\n",
    "            else:\n",
    "                assert(user_datum['c'] != -1)\n",
    "                assert(user_datum['r'] != -1)\n",
    "                view = int(user_datum['r'])\n",
    "                \n",
    "                if views.shape[0] <= sliding_window_depth or sliding_window_depth == -1:\n",
    "                    train_views = views\n",
    "                else:\n",
    "                    train_views = views[:sliding_window_depth, :]\n",
    "                if only_with_clicks:\n",
    "                    if user_datum['c'] != 0:\n",
    "                        train_X.append(train_views.sum(axis = 0))\n",
    "                        train_Y.append(view)\n",
    "                else:\n",
    "                    train_X.append(np.append(user_datum['r'], train_views.sum(axis = 0)))\n",
    "                    train_Y.append(user_datum['c'])\n",
    "\n",
    "    train_X = np.array(train_X).reshape(\n",
    "        len(train_Y), \n",
    "        number_of_products if only_with_clicks else number_of_products + 1\n",
    "    )\n",
    "    train_Y = np.array(train_Y)\n",
    "\n",
    "    return train_X, train_Y\n",
    "\n",
    "\n",
    "# Define agent class\n",
    "class ModelBasedBackLogistic:\n",
    "    def __init__(self, env, model, log_based = False):\n",
    "        # Set environment as an attribute of agent\n",
    "        self.env = env\n",
    "        self.model = model\n",
    "        self.log_based = log_based\n",
    "        self.number_of_products = self.env['num_products']\n",
    "        self.organic_views = np.zeros((self.number_of_products, self.number_of_products + 1))\n",
    "        self.organic_views[:, 0] = range(self.number_of_products)\n",
    "        \n",
    "    def train(self, observations, action, reward, done):\n",
    "        for observation in observations:\n",
    "            self.organic_views[:, observation[1] + 1] += 1\n",
    "\n",
    "    def offline_train(train):\n",
    "        pass\n",
    "    \n",
    "    def act(self, _ = None):\n",
    "        '''Act method returns an action based on current observation and past history'''\n",
    "        if self.log_based:\n",
    "            prob = self.model.predict_log_proba(self.organic_views)[:, 1]\n",
    "        else:\n",
    "            prob = self.model.predict_proba(self.organic_views)[:, 1]\n",
    "        action = np.argmax(prob)\n",
    "        return {\n",
    "            'a': action,\n",
    "            'ps': prob[action]\n",
    "        }\n",
    "    \n",
    "    def reset(self):\n",
    "        self.organic_views = np.zeros((self.number_of_products, self.number_of_products + 1))\n",
    "        self.organic_views[:, 0] = range(self.number_of_products)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this time, we are ready to launch our Agents and compare their IPSs with the default Agent that provides a Product randomly (with _Probability Score_ ~ 1/10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train X:  [[4. 0. 0. ... 0. 0. 0.]\n",
      " [4. 0. 0. ... 0. 0. 0.]\n",
      " [4. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [4. 0. 0. ... 0. 0. 1.]\n",
      " [9. 0. 0. ... 0. 0. 1.]\n",
      " [4. 0. 0. ... 0. 0. 1.]]\n",
      "Train Y:  [0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "train_X, train_Y = BuildLogisticRegressionData(a_data, env_1_args)\n",
    "print(\"Train X: \", train_X)\n",
    "print(\"Train Y: \", train_Y)\n",
    "\n",
    "model = LogisticRegression(\n",
    "    solver = 'lbfgs',\n",
    "    multi_class = 'multinomial',\n",
    "    max_iter = 1000\n",
    ").fit(train_X, train_Y)\n",
    "\n",
    "env.init_gym(env_1_args) # Reset the Environment\n",
    "b_data = env.generate_data(ABTestNumberOfUsers, ModelBasedBackLogistic(env_1_args, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    v  u  r  c        ps\n",
      "0   4  1 -1 -1       NaN\n",
      "1  -1  1  0  0  0.031206\n",
      "2  -1  1  0  0  0.031206\n",
      "3  -1  1  0  0  0.031206\n",
      "4  -1  1  0  0  0.031206\n",
      "5  -1  1  0  0  0.031206\n",
      "6  -1  1  0  0  0.031206\n",
      "7  -1  1  0  0  0.031206\n",
      "8  -1  1  0  0  0.031206\n",
      "9  -1  1  0  0  0.031206\n",
      "10 -1  1  0  0  0.031206\n",
      "11  4  1 -1 -1       NaN\n",
      "12  9  1 -1 -1       NaN\n",
      "13  5  1 -1 -1       NaN\n",
      "14 -1  1  0  0  0.031206\n",
      "15 -1  1  0  0  0.031136\n",
      "16 -1  1  0  0  0.031136\n",
      "17 -1  1  0  0  0.031136\n",
      "18 -1  1  0  0  0.031136\n",
      "19 -1  1  0  0  0.031136\n",
      "20  4  1 -1 -1       NaN\n",
      "21  4  1 -1 -1       NaN\n",
      "22  4  1 -1 -1       NaN\n",
      "23  4  1 -1 -1       NaN\n",
      "24 -1  1  0  0  0.031136\n"
     ]
    }
   ],
   "source": [
    "print(b_data[:25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** you shall see that _Organic_ events for cases *A* and *B* are similar. That is as it supposed to be because we are trying to test our new Agents within the same environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Inversed Probability Score_ is calculated as follow:\n",
    "$$ IPS = \\sum_{i \\in B} \\frac{\\pi_i^*(c|O)}{\\pi_i^r(c|O)} 1_c, $$\n",
    "where:\n",
    "* $ B $: a set of _Bandit_ events\n",
    "* $ \\pi_i^*(c|O) $: a new _Policy_\n",
    "* $ \\pi_i^r(c|O) $: an old _Random Policy_\n",
    "* $ 1_c $: one when by applying a new policy a click has been drawn\n",
    "* $ O $: observetions of _Products_; now, it is a vector that contains counters how many time each _Product_ was shown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of Bandit Events for Case A 145148\n",
      "Amount of Bandit Events for Case B 150300\n",
      "A Test Rewards:  1010\n",
      "B Test Rewards:  0\n",
      "A IPS vs. Random Choise:  43.52856073822884\n",
      "B IPS vs. Random Choise:  0.0\n"
     ]
    }
   ],
   "source": [
    "ABanditEvents = a_data[a_data['v'] == -1]\n",
    "print(\"Amount of Bandit Events for Case A\", len(ABanditEvents))\n",
    "ABanditEventsRewards = ABanditEvents[ABanditEvents['r'] == 1]\n",
    "\n",
    "BBanditEvents = b_data[b_data['v'] == -1]\n",
    "print(\"Amount of Bandit Events for Case B\", len(BBanditEvents))\n",
    "BBanditEventsRewards = BBanditEvents[BBanditEvents['r'] == 1]\n",
    "\n",
    "print(\"A Test Rewards: \", len(ABanditEventsRewards))\n",
    "print(\"B Test Rewards: \", len(BBanditEventsRewards))\n",
    "\n",
    "A_IPS = np.sum(ABanditEventsRewards['ps']) / float(env_1_args['num_products'])\n",
    "print(\"A IPS vs. Random Choise: \", A_IPS)\n",
    "\n",
    "B_IPS = np.sum(BBanditEventsRewards['ps']) / float(env_1_args['num_products'])\n",
    "print(\"B IPS vs. Random Choise: \", B_IPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you might see, the _Agent_ that is based on a Logistic Regression Model is less sensitive to data.\n",
    "Let's try to apply that agent but with _Log Probability_ and check the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of Bandit Events for Case C 150300\n",
      "C IPS vs. Random Choise:  0.0\n"
     ]
    }
   ],
   "source": [
    "env.init_gym(env_1_args) # Reset the Environment\n",
    "c_data = env.generate_data(ABTestNumberOfUsers, ModelBasedBackLogistic(env_1_args, model, True))\n",
    "\n",
    "\n",
    "CBanditEvents = c_data[c_data['v'] == -1]\n",
    "print(\"Amount of Bandit Events for Case C\", len(CBanditEvents))\n",
    "CBanditEventsRewards = CBanditEvents[CBanditEvents['r'] == 1]\n",
    "\n",
    "C_IPS = np.sum(CBanditEventsRewards['ps']) / float(env_1_args['num_products'])\n",
    "print(\"C IPS vs. Random Choise: \", C_IPS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
